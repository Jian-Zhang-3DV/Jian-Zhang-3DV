# Jian Zhang

<p align="right">
  <img src="jian_zhang.jpg" alt="Jian Zhang" style="border-radius: 50%; width: 150px; height: 150px; object-fit: cover;"/>
</p>

Graduate Student at Xiamen University, exploring Vision-Language Models (VLM) and 3D Perception.

## About Me

I am currently a graduate student at [Xiamen University](https://www.xmu.edu.cn/) (September 2023 - Present), supervised by Professor [Yue Huang](https://huangyue05.github.io/). I obtained my Bachelor's degree in Artificial Intelligence from [Nanchang University](http://www.ncu.edu.cn/) (September 2019 - June 2023).

My long-term research goal is to create AI agents capable of efficient industrial production in real-world environments, thereby significantly enhancing productivity.

**Contact:**
*   [Email](mailto:zjrandomyeah@gmail.com)
*   [GitHub](https://github.com/Jian-Zhang-3DV)
*   [Google Scholar](https://scholar.google.com/citations?user=qBNtBsAAAAAJ&hl=en&oi=sra)

## Publications

### VLM-3R: Vision-Language Models Augmented with Instruction-Aligned 3D Reconstruction (ArXiv, 2025)
*   Authors: **Jian Zhang***, Zhiwen Fan*, Renjie Li, Junge Zhang, Runjin Chen, Hezhen Hu, Kevin Wang, Huaizhi Qu, Dilin Wang, Zhicheng Yan, Hongyu Xu, Justin Theiss, Tianlong Chen, Jiachen Li, Zhengzhong Tu, Zhangyang Wang, Rakesh Ranjan (*Equal Contribution)
*   Abstract: This work introduces VLM-3R, a unified Vision-Language Model (VLM) framework incorporating 3D Reconstructive instruction tuning. VLM-3R processes monocular video to derive implicit 3D tokens, aligning spatial context with language instructions for 3D spatial assistance and embodied reasoning. It also introduces the Vision-Spatial-Temporal Intelligence benchmark.
*   Links: [üìÑ Paper](https://arxiv.org/abs/2505.20279) | [üíª Code](https://github.com/VITA-Group/VLM-3R) | [üåê Project Page](https://vlm-3r.github.io/)

### DynamicVerse: Physically-Aware Multimodal Modeling for Dynamic 4D Worlds (Preprint)
*   Authors: Kairun Wen*, Yuzhi Huang*, Runyu Chen, Hui Zheng, Yunlong Lin, Panwang Pan, Chenxin Li, Wenyan Cong, **Jian Zhang**, Junbin Lu, Chenguo Lin, Dilin Wang, Zhicheng Yan, Hongyu Xu, Justin Theiss, Yue Huang, Xinghao Ding, Rakesh Ranjan, Zhiwen Fan (*Equal Contribution)
*   Abstract: Understanding the dynamic physical world, characterized by its evolving 3D structure, real-world motion, and semantic content with textual descriptions, is crucial for human-agent interaction and enables embodied agents to perceive and act within real environments with human‚Äëlike capabilities. [...] DynamicVerse delivers a large-scale dataset consists of 100K+ videos with 800K+ annotated masks and 10M+ frames from internet videos.
*   Links: üìÑ Paper (Coming Soon) | üíª Code (Coming Soon) | [üåê Project Page](https://dynamic-verse.github.io/)

### Large Spatial Model: End-to-end Unposed Images to Semantic 3D (NeurIPS, 2024)
*   Authors: **Jian Zhang***, Zhiwen Fan*, Wenyan Cong, Peihao Wang, Renjie Li, Kairun Wen, Shijie Zhou, Achuta Kadambi, Zhangyang Wang, Danfei Xu, Boris Ivanovic, Marco Pavone, Yue Wang (*Equal Contribution)
*   Abstract: This work introduces the Large Spatial Model (LSM), which directly processes unposed RGB images into semantic radiance fields. LSM simultaneously estimates geometry, appearance, and semantics in a single feed-forward pass, achieving real-time semantic 3D reconstruction for the first time.
*   Links: [üìÑ Paper](https://arxiv.org/abs/2410.18956) | [üíª Code](https://github.com/NVlabs/LSM) | [üåê Project Page](https://largespatialmodel.github.io/)

### InstantSplat: Sparse-view Gaussian Splatting in Seconds (ArXiv, 2024)
*   Authors: Zhiwen Fan*, Kairun Wen*, Wenyan Cong*, Kevin Wang, **Jian Zhang**, Xinghao Ding, Danfei Xu, Boris Ivanovic, Marco Pavone, Georgios Pavlakos, Zhangyang Wang, Yue Wang (*Equal Contribution)
*   Abstract: While neural 3D reconstruction has advanced substantially, its performance significantly degrades with sparse-view data, which limits its broader applicability, since SfM is often unreliable in sparse-view scenarios where feature matches are scarce. In this paper, we introduce InstantSplat, a novel approach for addressing sparse-view 3D scene reconstruction at lightning-fast speed. InstantSplat employs a self-supervised framework that optimizes 3D scene representation and camera poses by unprojecting 2D pixels into 3D space and aligning them using differentiable neural rendering.
*   Links: [üìÑ Paper](https://arxiv.org/abs/2403.20309) | [üíª Code](https://github.com/NVlabs/InstantSplat) | [üåê Project Page](https://instantsplat.github.io/)
