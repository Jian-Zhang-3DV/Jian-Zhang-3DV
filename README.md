# 👋 Jian Zhang

<table>
<tr>
<td width="150">
<img src="jian_zhang.jpg" alt="Jian Zhang" width="150">
</td>
<td>
<em>🎓 Graduate Student at Xiamen University, exploring Vision-Language Models (VLM) and 3D Perception.</em>
<br><br>
🏫 I am currently a graduate student at <a href="https://www.xmu.edu.cn/">Xiamen University</a> (September 2023 - Present), supervised by Professor <a href="https://huangyue05.github.io/">Yue Huang</a>. I obtained my Bachelor's degree in Artificial Intelligence from <a href="http://www.ncu.edu.cn/">Nanchang University</a> (September 2019 - June 2023).
<br><br>
🤖 My long-term research goal is to create AI agents capable of efficient industrial production in real-world environments, thereby significantly enhancing productivity.
</td>
</tr>
</table>

**🔍 Looking for Opportunities:**
* 🌐 I am currently actively seeking research or engineering positions related to Multimodal Large Models.
* 🔬 I am particularly interested in opportunities where I can apply my research skills to real-world challenges and contribute to cutting-edge projects.
* 📩 If you have any potential collaborations or suitable job openings, please feel free to contact me.

**🔍 求职机会：**
* 🌐 我目前正在积极寻找多模态大模型相关的研究或工程岗位。
* 🔬 特别关注于提升模型在三维空间感知、理解、推理与规划方面的能力。
* 💼 我对能够将我的研究技能应用于实际挑战并为这些领域的前沿项目做出贡献的机会特别感兴趣。
* 📩 如果您有任何潜在的合作或合适的工作机会，请随时与我联系。

**📬 Contact:**
*   📧 [Email](mailto:zjrandomyeah@gmail.com)
*   🌐 [Homepage](https://jian-zhang-3dv.github.io/Jian-Zhang-3DV/)
*   📚 [Google Scholar](https://scholar.google.com/citations?user=qBNtBsAAAAAJ&hl=en&oi=sra)
*   📄 [CV](cv/CV_Jian_Zhang.pdf)

## 📝 Publications

### 🔥 VLM-3R: Vision-Language Models Augmented with Instruction-Aligned 3D Reconstruction (ArXiv, 2025)
*   **Authors:** **Jian Zhang***, Zhiwen Fan*, Renjie Li, Junge Zhang, Runjin Chen, Hezhen Hu, Kevin Wang, Huaizhi Qu, Dilin Wang, Zhicheng Yan, Hongyu Xu, Justin Theiss, Tianlong Chen, Jiachen Li, Zhengzhong Tu, Zhangyang Wang, Rakesh Ranjan (*Equal Contribution)
*   **Abstract:** This work introduces VLM-3R, a unified Vision-Language Model (VLM) framework incorporating 3D Reconstructive instruction tuning. VLM-3R processes monocular video to derive implicit 3D tokens, aligning spatial context with language instructions for 3D spatial assistance and embodied reasoning. It also introduces the Vision-Spatial-Temporal Intelligence benchmark.
*   **Links:** [📄 Paper](https://arxiv.org/abs/2505.20279) | [💻 Code](https://github.com/VITA-Group/VLM-3R) | [🌐 Project Page](https://vlm-3r.github.io/)

### 🌟 DynamicVerse: Physically-Aware Multimodal Modeling for Dynamic 4D Worlds (Preprint)
*   **Authors:** Kairun Wen*, Yuzhi Huang*, Runyu Chen, Hui Zheng, Yunlong Lin, Panwang Pan, Chenxin Li, Wenyan Cong, **Jian Zhang**, Junbin Lu, Chenguo Lin, Dilin Wang, Zhicheng Yan, Hongyu Xu, Justin Theiss, Yue Huang, Xinghao Ding, Rakesh Ranjan, Zhiwen Fan (*Equal Contribution)
*   **Abstract:** Understanding the dynamic physical world, characterized by its evolving 3D structure, real-world motion, and semantic content with textual descriptions, is crucial for human-agent interaction and enables embodied agents to perceive and act within real environments with human‑like capabilities. [...] DynamicVerse delivers a large-scale dataset consists of 100K+ videos with 800K+ annotated masks and 10M+ frames from internet videos.
*   **Links:** 📄 Paper (Coming Soon) | 💻 Code (Coming Soon) | [🌐 Project Page](https://dynamic-verse.github.io/)

### 🏆 Large Spatial Model: End-to-end Unposed Images to Semantic 3D (NeurIPS, 2024)
*   **Authors:** **Jian Zhang***, Zhiwen Fan*, Wenyan Cong, Peihao Wang, Renjie Li, Kairun Wen, Shijie Zhou, Achuta Kadambi, Zhangyang Wang, Danfei Xu, Boris Ivanovic, Marco Pavone, Yue Wang (*Equal Contribution)
*   **Abstract:** This work introduces the Large Spatial Model (LSM), which directly processes unposed RGB images into semantic radiance fields. LSM simultaneously estimates geometry, appearance, and semantics in a single feed-forward pass, achieving real-time semantic 3D reconstruction for the first time.
*   **Links:** [📄 Paper](https://arxiv.org/abs/2410.18956) | [💻 Code](https://github.com/NVlabs/LSM) | [🌐 Project Page](https://largespatialmodel.github.io/)

### ⚡ InstantSplat: Sparse-view Gaussian Splatting in Seconds (ArXiv, 2024)
*   **Authors:** Zhiwen Fan*, Kairun Wen*, Wenyan Cong*, Kevin Wang, **Jian Zhang**, Xinghao Ding, Danfei Xu, Boris Ivanovic, Marco Pavone, Georgios Pavlakos, Zhangyang Wang, Yue Wang (*Equal Contribution)
*   **Abstract:** While neural 3D reconstruction has advanced substantially, its performance significantly degrades with sparse-view data, which limits its broader applicability, since SfM is often unreliable in sparse-view scenarios where feature matches are scarce. In this paper, we introduce InstantSplat, a novel approach for addressing sparse-view 3D scene reconstruction at lightning-fast speed. InstantSplat employs a self-supervised framework that optimizes 3D scene representation and camera poses by unprojecting 2D pixels into 3D space and aligning them using differentiable neural rendering.
*   **Links:** [📄 Paper](https://arxiv.org/abs/2403.20309) | [💻 Code](https://github.com/NVlabs/InstantSplat) | [🌐 Project Page](https://instantsplat.github.io/)
