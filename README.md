# Hi there, I'm Jian Zhang! ðŸ‘‹

<img src="jian_zhang.jpg" alt="Jian Zhang" width="200" height="auto" style="border-radius: 50%;" />

## About Me ðŸš€

I'm a second-year graduate student at Xiamen University (XMU), focusing on Vision-Language Models (VLM) and 3D perception research.

## Publications ðŸ“„

*   **InstantSplat: Sparse-view Gaussian Splatting in Seconds** - *ArXiv* ([https://instantsplat.github.io/](https://instantsplat.github.io/))
    *   Authors: Zhiwen Fan\*, Kairun Wen\*, Wenyan Cong\*, Kevin Wang, **Jian Zhang**, Xinghao Ding, Danfei Xu, Boris Ivanovic, Marco Pavone, Georgios Pavlakos, Zhangyang Wang, Yue Wang (\*Equal Contribution)
    *   Abstract: While neural 3D reconstruction has advanced substantially, its performance significantly degrades with sparse-view data, which limits its broader applicability, since SfM is often unreliable in sparse-view scenarios where feature matches are scarce. In this paper, we introduce InstantSplat, a novel approach for addressing sparse-view 3D scene reconstruction at lightning-fast speed. InstantSplat employs a self-supervised framework that optimizes 3D scene representation and camera poses by unprojecting 2D pixels into 3D space and aligning them using differentiable neural rendering.
    *   [ðŸ“¹ Video Demo](projects/instantsplat/demo1.mp4)
    
*   **Large Spatial Model: End-to-end Unposed Images to Semantic 3D** - *NeurIPS 2024* ([https://largespatialmodel.github.io/](https://largespatialmodel.github.io/))
    *   Authors: Zhiwen Fan\*, **Jian Zhang**\*, Wenyan Cong, Peihao Wang, Renjie Li, Kairun Wen, Shijie Zhou, Achuta Kadambi, Zhangyang Wang, Danfei Xu, Boris Ivanovic, Marco Pavone, Yue Wang (\*Equal Contribution)
    *   Abstract: This work introduces the Large Spatial Model (LSM), which directly processes unposed RGB images into semantic radiance fields. LSM simultaneously estimates geometry, appearance, and semantics in a single feed-forward pass, achieving real-time semantic 3D reconstruction for the first time.
    *   [ðŸ“¹ Video Demo](projects/lsm/LSM_demo.mp4)
*   **VLM-3R: Vision-Language Models Augmented with Instruction-Aligned 3D Reconstruction** - *ArXiv* ([https://vlm-3r.github.io/](https://vlm-3r.github.io/))
    *   Authors: Zhiwen Fan\*, **Jian Zhang**\*, Renjie Li, Junge Zhang, Runjin Chen, Hezhen Hu, Kevin Wang, Huaizhi Qu, Dilin Wang, Zhicheng Yan, Hongyu Xu, Justin Theiss, Tianlong Chen, Jiachen Li, Zhengzhong Tu, Zhangyang Wang, Rakesh Ranjan (\*Equal Contribution)
    *   Abstract: This work introduces VLM-3R, a unified Vision-Language Model (VLM) framework incorporating 3D Reconstructive instruction tuning. VLM-3R processes monocular video to derive implicit 3D tokens, aligning spatial context with language instructions for 3D spatial assistance and embodied reasoning. It also introduces the Vision-Spatial-Temporal Intelligence benchmark.
    *   [ðŸ“¹ Video Demo](projects/vlm3r/arc-dynamic.mp4)

<!-- Add more publications as needed -->

## Get in Touch ðŸ“¬

- ðŸŽ“ Google Scholar: **[https://scholar.google.com/citations?user=qBNtBsAAAAAJ&hl=en&oi=sra](https://scholar.google.com/citations?user=qBNtBsAAAAAJ&hl=en&oi=sra)**
